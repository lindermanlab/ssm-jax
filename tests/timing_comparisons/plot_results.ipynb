{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935b7eaf",
   "metadata": {},
   "source": [
    "# Notebook to Generate Benchmark Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22f7e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "43a8ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_results_to_df(json_results_fname):\n",
    "\n",
    "    # read in json from fname\n",
    "    with open(json_results_fname, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # construct dataframe\n",
    "    df = pd.DataFrame(json_data[\"benchmarks\"])\n",
    "    df = pd.concat([df, df[\"stats\"].apply(pd.Series)], axis=1)\n",
    "    df[\"test_name\"] = df[\"name\"].apply(lambda x: x.split(\"[\")[0])\n",
    "    df[\"param_name\"] = df[\"params\"].apply(lambda x: list(x.keys())[0])\n",
    "    df[\"param_value\"] = df[\"params\"].apply(lambda x: list(x.values())[0])\n",
    "\n",
    "    # select columns to keep\n",
    "    columns = [\n",
    "        \"test_name\",\n",
    "        \"param_name\",\n",
    "        \"param_value\",\n",
    "        \"min\",\n",
    "        \"max\",\n",
    "        \"mean\",\n",
    "        \"median\",\n",
    "        \"stddev\",\n",
    "        \"ops\",\n",
    "        \"iterations\",\n",
    "        \"rounds\",\n",
    "        \"extra_info\",\n",
    "        \"params\",\n",
    "    ]\n",
    "    df = df[columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_series(x, y, color=\"C0\", label=\"\"):\n",
    "    params = x\n",
    "    mean_times = y\n",
    "    isnan = np.isnan(mean_times)\n",
    "    if np.sum(~isnan) > 1:\n",
    "        m, b = np.polyfit(params[~isnan], mean_times[~isnan], deg=1)\n",
    "        plt.plot(params, m * params + b, \"--\", color=color, label=label)\n",
    "    else:\n",
    "        m = 0\n",
    "        b = mean_times[~isnan]\n",
    "        plt.plot([], [], \"--\", color=color, label=label)\n",
    "    ys = np.copy(mean_times)\n",
    "    ys[isnan] = m * params[isnan] + b  # replace nans with linear prediction\n",
    "    for x, y, isna in zip(params, ys, isnan):\n",
    "        plt.scatter(\n",
    "            x, y, marker=\"x\" if isna else \"o\", s=80 if isna else 40, color=color\n",
    "        )\n",
    "\n",
    "\n",
    "def load_multiple_runs(benchmark_runs):\n",
    "    runs = []\n",
    "    for run_name, run_fname in benchmark_runs.items():\n",
    "        df = parse_json_results_to_df(run_fname)\n",
    "        cols = df.columns\n",
    "        cols = cols.insert(0, \"run_name\")\n",
    "        df[\"run_name\"] = run_name\n",
    "        df = df[cols]\n",
    "        runs.append(df)\n",
    "    df = pd.concat(runs)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_across_params(runs_df, show_plots=False, save_to_pdf: bool or str = False):\n",
    "    if save_to_pdf:\n",
    "        pdf = PdfPages(save_to_pdf)\n",
    "    for test_name, test_grp in runs_df.groupby([\"test_name\"]):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        colors = [\"green\", \"blue\", \"purple\", \"orange\", \"red\"]\n",
    "        # plot time vs. params for different runs (as different series)\n",
    "        for color, (run_name, run_grp) in zip(colors, test_grp.groupby([\"run_name\"])):\n",
    "            plot_series(\n",
    "                run_grp[\"param_value\"], run_grp[\"mean\"], label=run_name, color=color\n",
    "            )\n",
    "        plt.title(test_name)\n",
    "        plt.xlabel(run_grp[\"param_name\"].iloc[0])\n",
    "        plt.ylabel(\"time (s)\")\n",
    "        ax = plt.gca()\n",
    "        box = ax.get_position()  # Shink current axis by 20% to fit legend in pdf\n",
    "        ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "        plt.legend(bbox_to_anchor=(1, 1))\n",
    "        if save_to_pdf:\n",
    "            pdf.savefig()\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "    if save_to_pdf:\n",
    "        pdf.close()\n",
    "\n",
    "\n",
    "def plot_across_runs(runs_df, show_plots=False, save_to_pdf: bool or str = False):\n",
    "    if save_to_pdf:\n",
    "        pdf = PdfPages(save_to_pdf)\n",
    "    colors = [\"green\", \"blue\", \"purple\", \"orange\", \"red\"][::-1]\n",
    "    for i, (test_name, test_grp) in enumerate(runs_df.groupby([\"test_name\"])):\n",
    "        for param_value, param_grp in test_grp.groupby([\"param_value\"]):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            run_names = param_grp[\"run_name\"][::-1]\n",
    "            height = np.nan_to_num(np.array(param_grp[\"mean\"]), -1)[::-1]\n",
    "            plt.bar(x=run_names, height=height, label=param_grp, color=colors)\n",
    "            plt.title(f\"{test_name} [{param_value}]\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel(\"time (s)\")\n",
    "            ax = plt.gca()\n",
    "            box = ax.get_position()  # Shink current axis by 20% to fit xlabels in pdf\n",
    "            ax.set_position([box.x0, box.y0 + (box.height * 0.2), box.width, box.height * 0.8])\n",
    "            if save_to_pdf:\n",
    "                pdf.savefig()\n",
    "            if show_plots:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "    if save_to_pdf:\n",
    "        pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_runs = {\n",
    "    \"log normalizer (latest)\": \"/Users/collinschlager/Code/ssm-jax-refactor/tests/timing_comparisons/.benchmarks/Darwin-CPython-3.9-64bit/0006_use_log_normalizer.json\",\n",
    "    \"discrete chain (prev1)\": \"/Users/collinschlager/Code/ssm-jax-refactor/tests/timing_comparisons/.benchmarks/Darwin-CPython-3.9-64bit/0004_scott-refactor.json\",\n",
    "    \"m_step_refactor (prev2)\": \"/Users/collinschlager/Code/ssm-jax-refactor/tests/timing_comparisons/.benchmarks/Darwin-CPython-3.9-64bit/0005_main_pre_refactor.json\",\n",
    "    \"components (prev3)\": \"/Users/collinschlager/Code/ssm-jax-refactor/tests/timing_comparisons/.benchmarks/Darwin-CPython-3.9-64bit/0003_pre_hmm_refactor.json\",\n",
    "    \"ssm_v0\": \"/Users/collinschlager/Code/ssm-jax-refactor/tests/timing_comparisons/ssm_v0_benchmark_tests/.benchmarks/Darwin-CPython-3.9-64bit/0002_ssm-v0-hmm.json\",\n",
    "}\n",
    "\n",
    "runs_df = load_multiple_runs(benchmark_runs)\n",
    "plot_across_params(runs_df, save_to_pdf=\"timing_report_A.pdf\")\n",
    "plot_across_runs(runs_df, save_to_pdf=\"timing_report_B.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6376f36d2a576cb3aa96bdc8604fd3c48ab4175c4f0714c2dd2fe4ec268050b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ssmjax': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
