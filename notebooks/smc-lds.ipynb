{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcc7833",
   "metadata": {},
   "source": [
    "# SMC in (Gaussian) LDS Notebook\n",
    "\n",
    "This notebook demonstrates how to use the SMC inference procedure included in JAX-SSM.  \n",
    "\n",
    "SMC is sampling algorithm for sampling from a posterior over latent states under a fixed model:\n",
    "\n",
    "$\\mathbf{x}^{(i)} \\sim p(\\cdot | \\mathbf{y}, \\theta)$\n",
    "\n",
    "where the latent state is denoted $\\mathbf{x} \\in \\mathcal{X}$, the observed data is denoted $\\mathbf{y} \\in \\mathcal{Y}$, and the model is fixed, indicated by conditioning on a fixed model (or set of model parameter values) $\\theta$.  Individual particles are denoted by a superscripted+bracketed index.\n",
    "\n",
    "Note that while the SMC implementation does support arbitrary, in this simple example we will use the model itself as the proposal, and weight under the likelihood, in what is referred to as a _bootstrap particle filter_ (BPF).  The BPF is arguably the \"default\" manifestation of SMC.\n",
    "\n",
    "SMC can also produce an unbiased estimate of the _marginal likelihood_ (ML, also referred to as the _evidence_, or simply the _marginal_), $p(\\mathbf{y} | \\theta)$.  The LML is a critical and quantative measure of how well the model describes the observed data.\n",
    "\n",
    "Since the Gaussian LDS can be solved in closed-form and admits Gaussian posteriors, EM will produce an exact evaluation of the evidence and will provide exact posterior distributions.  Therefore, we compare to EM to verify:\n",
    "\n",
    "- The (qualitative) quality of inferred posteriors over latent state.\n",
    "- The unbiasedness of the estimate of the marginal likelihood.\n",
    "- TODO - The variance of the estimate of the marginal likelihood reduces as $\\sqrt{N}$, where $N$ is the number of particles used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fcd350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import importlib\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.scipy as jscipy\n",
    "import ssm.utils as util\n",
    "\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "# SSM imports.\n",
    "from ssm.lds.models import GaussianLDS\n",
    "from ssm.inference.smc import smc\n",
    "from ssm.inference.smc import _plot_single_sweep\n",
    "\n",
    "# # For testing code:\n",
    "# import ssm.inference.test_smc as test_smc\n",
    "# importlib.reload(test_smc)\n",
    "\n",
    "# Set the initial PRNG key.\n",
    "key = jax.random.PRNGKey(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0213046",
   "metadata": {},
   "source": [
    "# 1. Create Gaussian LDS and Observed Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4055d5a",
   "metadata": {},
   "source": [
    "#### Set up true model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613face",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "emissions_dim = 3\n",
    "\n",
    "gen_model = lambda _k: GaussianLDS(num_latent_dims=latent_dim, num_emission_dims=emissions_dim, seed=_k, \n",
    "                                   dynamics_scale_tril=0.1 * jnp.eye(latent_dim),\n",
    "                                   emission_scale_tril=1.0 * jnp.eye(emissions_dim),\n",
    "                                   # initial_state_scale_tril=0.1 * jnp.eye(latent_dim)\n",
    "                                  )\n",
    "model = gen_model(jax.random.PRNGKey(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c799ba6",
   "metadata": {},
   "source": [
    "#### From the true model, we can sample synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f574b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10\n",
    "num_timesteps = 100\n",
    "\n",
    "true_states, data = model.sample(key=jax.random.PRNGKey(4), num_steps=num_timesteps, num_samples=num_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653d6c9",
   "metadata": {},
   "source": [
    "#### Let's view the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5769bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_plot = 0\n",
    "fig, axes = plt.subplots(2, 1, sharex=True, squeeze=True)\n",
    "axes[0].plot(true_states[dataset_to_plot])\n",
    "axes[0].grid(True)\n",
    "axes[0].set_ylabel('$x_t$')\n",
    "axes[1].plot(data[dataset_to_plot])\n",
    "axes[1].grid(True)\n",
    "axes[1].set_ylabel('$y_t$')\n",
    "plt.xlabel('Timestep ($t$)')\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77896d",
   "metadata": {},
   "source": [
    "# 2. Inference Over Latent State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09156f",
   "metadata": {},
   "source": [
    "#### Inference baseline: Use EM to infer the exact posterior over latent state given observes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test against EM (which for the LDS is exact.\n",
    "em_posterior = jax.vmap(model.e_step)(data)\n",
    "em_log_marginal_likelihood = model.marginal_likelihood(data, posterior=em_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845dcef",
   "metadata": {},
   "source": [
    "#### Inference: Infer the posterior over latent state given observes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc11e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 1000  # Number of particles to use in SMC sweeps.\n",
    "n_repeats = 20        # Number of repeats for each dataset (independent SMC trials).\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "# Define a wrapper for SMC sweep using BPF.\n",
    "repeat_smc = lambda _k: smc(_k, model, data, proposal=None, num_particles=num_particles)\n",
    "\n",
    "# Compute the posteriors.\n",
    "smc_posteriors = jax.vmap(repeat_smc)(jax.random.split(subkey, num=n_repeats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160a90c",
   "metadata": {},
   "source": [
    "#### Plot the results of inference for a single sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(test_smc)  # Quick little hack to re-load the plotting scripts.\n",
    "\n",
    "# Plot the results.\n",
    "# Which repeat and dataset to plot.\n",
    "rep = 0\n",
    "dset = 0\n",
    "\n",
    "# Pull out the right sweeps and compile statistics.\n",
    "sweep_true = true_states[dset]\n",
    "sweep_smc_filtering = smc_posteriors[rep, dset].filtering_particles\n",
    "sweep_smc_smoothing = smc_posteriors[rep, dset].sample(sample_shape=num_particles, seed=subkey)\n",
    "sweep_em_mean = em_posterior.mean()[dset]\n",
    "sweep_em_sds = jnp.sqrt(jnp.asarray([[jnp.diag(__k) for __k in _k] for _k in em_posterior.covariance()]))[dset]\n",
    "sweep_em_statistics = (sweep_em_mean, sweep_em_mean - sweep_em_sds, sweep_em_mean + sweep_em_sds)\n",
    "\n",
    "# Plot the SMC and EM distributions.\n",
    "_plot_single_sweep(sweep_smc_filtering, sweep_true, tag='SMC filtering')\n",
    "_plot_single_sweep(sweep_smc_smoothing, sweep_true, tag='SMC smoothing')\n",
    "_plot_single_sweep(sweep_em_statistics, sweep_true, tag='EM smoothing', preprocessed=True)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27684e7",
   "metadata": {},
   "source": [
    "# 3. Verify ML Estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d08b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quickly just show some of the estimates.\n",
    "smc_posteriors.log_normalizer[:, 0], em_log_marginal_likelihood[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea037f",
   "metadata": {},
   "source": [
    "This box-and-whisker plot shows the log-marginal likelihoods, as they are generally better scaled and easier to visualize.  Note however that SMC does not produce an unbiased estimate of the log marginal likelihoods (c.f. Jensens inequality), and therefore this is just for back-of-the-envelope verification that the LML estimations are sensible...\n",
    "\n",
    "The boxes are over the estimate of the LML generated by SMC (of which there are `n_repeats` individual estimates) for each trial (of which there are `num_trials`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2af808",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.boxplot(smc_posteriors.log_normalizer.T, showfliers=False)\n",
    "plt.grid(True)\n",
    "plt.scatter(jnp.arange(len(smc_posteriors.log_normalizer.T)) + 1, em_log_marginal_likelihood, label='EM')\n",
    "plt.legend()\n",
    "plt.ylabel('Log marginal likelihood')\n",
    "plt.xlabel('Trial')\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the expected log marginals for SMC.\n",
    "smc_elml = util.lexp(smc_posteriors.log_normalizer, _axis=0)\n",
    "print(smc_elml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('|-----------------------------------------|')\n",
    "print('| Trial, i: |    log E [p(y_i | theta)]   |')\n",
    "print('|           |-----------------------------|')\n",
    "print('|           |      SMC     |      EM      |')\n",
    "print('|-----------|--------------|--------------|')\n",
    "for _idx, _smc, _em in zip(range(len(smc_elml)), smc_elml, em_log_marginal_likelihood):\n",
    "    print('|   {: >3d}  '.format(_idx),\n",
    "          '  |  {: >8.2f} '.format(_smc), \n",
    "          '  |  {: >8.2f} '.format(_em), \n",
    "          '  |  ')\n",
    "print('|-----------------------------------------|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd3432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lml_diff = smc_elml - em_log_marginal_likelihood\n",
    "print('Raw difference in LML: ', '  '.join(['{: >5.2f}'.format(_x) for _x in lml_diff]))\n",
    "\n",
    "ml_diff = jnp.exp(lml_diff)\n",
    "print('Raw ratios of ML:      ', '  '.join(['{: >5.2f}'.format(_x) for _x in ml_diff]))\n",
    "\n",
    "mml_diff = jnp.mean(ml_diff)\n",
    "print()\n",
    "print('Expected ratio in ML:  ', mml_diff)\n",
    "print('(should =1, <1 implies evidence is underestimated, >1 implies evidence is overestimated.)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be7a3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de73a9e",
   "metadata": {},
   "source": [
    "# Experimental Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a90fa",
   "metadata": {},
   "source": [
    "# A. Analyse convergence of ML estimator across particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93800168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_repeats = 20                              # Number of repeats for each dataset (independent trials).\n",
    "# vec_num_particles = jnp.asarray([10, 50, 100, 500, 1000])  # Number of particles to use in SMC sweeps.\n",
    "\n",
    "# key, subkey = jax.random.split(key)\n",
    "\n",
    "# # Define a wrapper for SMC sweep using BPF.\n",
    "# repeat_smc_wk_wp = lambda __k, __p: smc(__k, model, data, proposal=None, num_particles=__p)\n",
    "\n",
    "# # Note that because different numbers of particles entail different variable\n",
    "# # dimensions at execution time, we need to explicitly iterate over the different\n",
    "# # numbers of particles.\n",
    "# particle_lml = []\n",
    "# for _p in vec_num_particles:\n",
    "#     key, subkey = jax.random.split(key)\n",
    "#     _lmls = jax.vmap(repeat_smc_wk_wp, in_axes=(0, None))(jax.random.split(subkey, num=n_repeats), _p)\n",
    "#     particle_lml.append(_lmls[1])\n",
    "\n",
    "# particle_lml = jnp.asarray(particle_lml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lml_error = particle_lml - em_log_marginal_likelihood\n",
    "# lml_variance = jnp.mean(jnp.var(jnp.exp(error) - 1, axis=1), axis=1)\n",
    "\n",
    "# plt.plot(vec_num_particles, lml_variance)\n",
    "# # plt.yscale('log')\n",
    "\n",
    "# print(lml_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02624af7",
   "metadata": {},
   "source": [
    "# B. Verify ML Estimations across models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b90b7",
   "metadata": {},
   "source": [
    "What we did above was conditioned on a single model ($\\theta$), and so we need to repeat this estimation across models.  We will therefore average over trials and repeats, conditioned on individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_single_model(key):\n",
    "    \n",
    "#     # Generate a new model and data.\n",
    "#     key, subkey = jax.random.split(key)\n",
    "#     model = gen_model(subkey)\n",
    "#     key, subkey = jax.random.split(key)\n",
    "#     true_states, data = model.sample(key=subkey, num_steps=num_timesteps, num_samples=num_trials)\n",
    "\n",
    "#     # Test against EM (which for the LDS is exact.\n",
    "#     em_posterior = jax.vmap(model.e_step)(data)\n",
    "#     em_log_marginal_likelihood = model.marginal_likelihood(data, posterior=em_posterior)\n",
    "\n",
    "#     # Compute the posteriors using SMC.\n",
    "#     key, subkey = jax.random.split(key)\n",
    "#     repeat_smc = lambda _k: smc(_k, model, data, proposal=None, num_particles=num_particles)\n",
    "#     smc_posteriors = jax.vmap(repeat_smc)(jax.random.split(subkey, num=n_repeats))\n",
    "#     log_marginal_likelihood = smc_posteriors.log_normalizer\n",
    "    \n",
    "#     return log_marginal_likelihood, em_log_marginal_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d799ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_timesteps = 100    # Number of timesteps in each trial.\n",
    "# num_particles = 100    # Number of particles to use in SMC sweeps.\n",
    "# num_trials = 20        # Number of trials to draw per model.\n",
    "# n_repeats = 10         # Number of repeats for each dataset (independent SMC runs).\n",
    "# n_models_to_test = 10   # Number of random models to test.\n",
    "\n",
    "# # Sometimes vmap causes the swap file to be thrashed. \n",
    "# smc_lmls, em_lmls = [], []\n",
    "# for _k in jax.random.split(key, num=n_models_to_test):\n",
    "#     key, subkey = jax.random.split(key)\n",
    "#     ret = test_single_model(_k)\n",
    "#     smc_lmls.append(ret[0])\n",
    "#     em_lmls.append(ret[1])\n",
    "# smc_lmls = jnp.asarray(smc_lmls)\n",
    "# em_lmls = jnp.asarray(em_lmls)\n",
    "\n",
    "# # key, subkey = jax.random.split(key)\n",
    "# # smc_lmls, em_lmls = jax.vmap(test_single_model)(jax.random.split(key, num=n_models_to_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_smc_lml = jscipy.special.logsumexp(smc_lmls, axis=(1, 2)) - jnp.log(jnp.prod(jnp.asarray(smc_lmls.shape[1:])))\n",
    "# mean_em_lml = jscipy.special.logsumexp(em_lmls, axis=1) - jnp.log(jnp.prod(jnp.asarray(em_lmls.shape[1:])))\n",
    "\n",
    "# # TODO - not quite sure whether this is a sensible thing to chart.\n",
    "\n",
    "# print('|-------------------------------------|')\n",
    "# print('| Model, i: | log E [ p( theta_i ) ]  |')\n",
    "# print('|           |-------------------------|')\n",
    "# print('|           |     SMC    |     EM     |')\n",
    "# print('|-----------|------------|------------|')\n",
    "# for _idx, _smc, _em in zip(range(len(mean_smc_lml)), mean_smc_lml, mean_em_lml):\n",
    "#     print('|   {: >3d}  '.format(_idx),\n",
    "#           '  |  {: >5.2f}'.format(_smc), \n",
    "#           '  |  {: >5.2f}'.format(_em), \n",
    "#           '  |  ')\n",
    "# print('|-------------------------------------|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cb812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lml_diff = mean_smc_lml - mean_em_lml\n",
    "# print('Raw difference in LML: ', '  '.join(['{: >5.2f}'.format(_x) for _x in lml_diff]))\n",
    "\n",
    "# ml_diff = jnp.exp(lml_diff)\n",
    "# print('Raw ratios of ML:      ', '  '.join(['{: >5.2f}'.format(_x) for _x in ml_diff]))\n",
    "\n",
    "# mml_diff = jnp.mean(ml_diff)\n",
    "# print()\n",
    "# print('Expected ratio in ML:  ', mml_diff)\n",
    "# print('(should =1, <1 implies evidence is underestimated, >1 implies evidence is overestimated.)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f93fea",
   "metadata": {},
   "source": [
    "# C - Random Stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smc_posteriors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
